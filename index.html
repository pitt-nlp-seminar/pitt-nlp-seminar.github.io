<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>University of Pittsburgh NLP Seminar</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="" />
<meta name="author" content="http://webthemez.com" />
<!-- css -->
<link href="css/bootstrap.min.css" rel="stylesheet" />
<link href="css/fancybox/jquery.fancybox.css" rel="stylesheet">
<link href="css/jcarousel.css" rel="stylesheet" />
<link href="css/flexslider.css" rel="stylesheet" />
<link href="js/owl-carousel/owl.carousel.css" rel="stylesheet">
<link href="css/style.css" rel="stylesheet" />

<!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
<!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

</head>
<body>
<div id="wrapper">
	<!-- start header -->
	<header>
	    <div class="container">
		 <a class="text-center" href="index.html">
       <img src="img/University_of_Pittsburgh_Logo_RGB_Primary_3-Color.png" alt="logo" style="width:25%;height:auto;padding-bottom:10px;margin-bottom:10px;margin-top:10px;margin-right:50px"/>
		  </a>
      <strong style="font-size:16pt;margin-left:10%">Natural Language Processing Seminar</strong>
	    </div>
	</header>
	<!-- end header -->
	<section id="inner-headline">
	</section>

	<section id="content">


	<div class="container" >
		<!-- ################### Current Speakers ################### -->
		<div style="text-align:center;font-size:16pt;font-weight:bold">
			Winter 2025
		</div>
		<p><a href="https://docs.google.com/document/d/1qJyrbX2H25Y3xzF626G3-k-x4K1FA5Y-6r54rGnRI6o/edit?usp=sharing">Subscribe for updates on Pitt NLP Seminar</a></p>
		<hr>
		<!-- ################### Current Speakers ################### -->

		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2025_spring/Akshat-Gupta.png">
				</div>
				<div>
					<h3> <a href="https://akshat57.github.io/">Akshat Gupta</a>  </h3>
					<p><b> UC Berkeley </b></p>
					<p><b> Time:</b> 03/12/2025, 4:30 - 5:30 PM (EST)</p>
					<p><b> Place:</b> <a href="https://pitt.zoom.us/j/95500691262">Zoom</a> </p>
					
					<p><b>Bio:</b> 
						Akshat Gupta is a second year CS PhD student at UC Berkeley affiliated with BAIR and Berkeley Speech Group, advised by Prof. Gopala Anumanchipalli. Before joining Berkeley, He spent two wonderful years at AI Research, JPMorgan where he worked as an NLP Research Engineer.

						Akshat Gupta graduated from Carnegie Mellon University (MS), where he was advised by Prof. Alan Black. He did his Masters in Physics from Technical University of Munich, Germany, and his thesis was advised by Prof. Gregory Eyink at Johns Hopkins University.
					</p>
					<!-- <p><b>Title:</b> 
					
					</p>
					<p><b>Abstract:</b> 

					</p> -->
				</div>
			</div>
		</div>

		<hr>
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2025_spring/anjalie-field.jpg">
				</div>
				<div>
					<h3> <a href="https://anjalief.github.io/">Anjalie Field </a></h3>
					<p><b> Johns Hopkins University </b></p>
					<p><b> Time:</b> 03/19/2025, 4:30 - 5:30 PM (EST)</p>
					<p><b> Place:</b> In-person (SQ 6106) </p>
					
					<!-- <p><b>Bio:</b> 
						I am an Assistant Professor in the Computer Science Department, Whiting School of Engineering at Johns Hopkins University. I am also affiliated with the Center for Language and Speech Processing (CLSP). My research focuses on the ethics and social science aspects of natural language processing, which includes developing language processing models to address societal issues like discrimination and propaganda, as well as critically assessing AI pipelines. Prior to joining JHU, I was a postdoctoral researcher in the Stanford NLP Group and Stanford Data Science Institute working with Dan Jurafsky and Jennifer Eberhardt. I completed my PhD at the Language Technologies Institute at Carnegie Mellon University, where I was advised by Yulia Tsvetkov.
					</p> -->
					<!-- <p><b>Title:</b> 
					
					</p>
					<p><b>Abstract:</b> 

					</p> -->
				
				</div>
			</div>
		</div>

		<hr>
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2025_spring/qifan_wang.jpg">
				</div>
				<div>
					<h3> <a href="https://wqfcr.github.io/">Qifan Wang </a>  </h3>
					<p><b> MetaAI </b></p>
					<p><b> Time:</b> 03/26/2025, 4:30 - 5:30 PM (EST)</p>
					<p><b> Place:</b> Zoom </p>
					
					<!-- <p><b>Title:</b> 
					
					</p>
					<p><b>Abstract:</b> 

					</p> -->
				
				</div>
			</div>
		</div>

		<hr>
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2025_spring/michael.jpg">
				</div>
				<div>
					<h3><a href="https://michaelmilleryoder.github.io/">Michael Miller Yoder</a>   </h3>
					<p><b> University of Pittsburgh </b></p>
					<p><b> Time:</b> 04/09/2025, 4:30 - 5:30 PM (EST)</p>
					<p><b> Place:</b> In-person </p>
					
					<p><b>Bio:</b> 
						Michael Miller Yoder is a Teaching Assistant Professor in the School of Computing and Information at the University of Pittsburgh. His teaching and research focus on data science, natural language processing, and computational social science. His work generally applies computational text analysis and other quantitative approaches to study social interaction, including how identities and ideologies are expressed. He is also interested in the social and ethical implications of data science and computational technologies.
					</p>
					<!-- <p><b>Title:</b> 
					
					</p>
					<p><b>Abstract:</b> 

					</p> -->
				
				</div>
			</div>
		</div>

		<hr>
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2025_spring/jieyu-zhao.jpg">
				</div>
				<div>
					<h3> <a href="https://jyzhao.net/">Jieyu Zhao</a> </h3> 
					<p><b> University of Southern California </b></p>
					<p><b> Time:</b> 04/16/2025, 4:30 - 5:30 PM (EST)</p>
					<p><b> Place:</b> Zoom </p>
					
					<p><b>Bio:</b> 
						Jieyu Zhao is an Assistant Professor of Computer Science Department at University of Southern California. Prior to that, she was an NSF Computing Innovation Fellow at University of Maryland, College Park, working with Prof. Hal Daumé III. Jieyu received her Ph.D. from Computer Science Department, UCLA, where she was advised by Prof. Kai-Wei Chang. Her research interest lies in fairness of ML/NLP models. Her paper got the EMNLP Best Long Paper Award (2017). She was one of the recipients of 2020 Microsoft PhD Fellowship and has been selected to participate in 2021 Rising Stars in EECS workshop. Her research has been covered by news media such as Wired, The Daily Mail and so on. She was invited by UN-WOMEN Beijing on a panel discussion about gender equality and social responsibility. 
					</p>
					<!-- <p><b>Title:</b> 
					
					</p>
					<p><b>Abstract:</b> 

					</p>-->
				 
				</div>
			</div>
		</div>
		

		


		<!-- ################### Previous Speakers ################### -->
		<hr>
		<hr>
		<div style="text-align:center;font-size:16pt;font-weight:bold">
			Previous Speakers
		</div>
		<hr>
		<hr>
		<!-- ################### Previous Speakers ################### -->


		
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2024_fall/raquel_coelho_2024.webp">
				</div>
				<div>
					<h3> Raquel Coelho </h3>
					<p><b> University of Pittsburgh </b></p>
					<p><b> Time:</b> 09/25/2024, 15:00PM EST - 16:00PM EST</p>
					<p><b> Place:</b> In-person (Location TBD) </p>
					<p><b>Bio:</b> Rachel Coelho is jointly appointed as a Research Scientist at the Learning Research and Development Center. She holds a PhD in Learning Sciences and Technology Design combined with Education Data Science from Stanford University. Her research, rooted in sociocultural theories of learning, examines novel applications of text analytics and text generation technologies in educational contexts.</p>
					<p><b>Title:</b> Conversational Dynamics with Large Language Model-powered Chatbots: Unknown Unknowns, Suspended Conclusions, and Benefits 
					<p><b>Abstract:</b> This study focuses on learner reports about learning conversations to understand differences in conversation dynamics between LLM chatbots and familiar human interactions. Sociocultural theory argues talk-in-interaction between humans facilitates knowledge construction. LLM-powered chatbots are hypothesized to enhance learner meaning-making and knowledge construction. For working on lab session multiple-choice tests (MCTs), 96 students were assigned to one of three conditions: no support, peer support, or ChatGPT support. After five MCTs, they self-selected into a different group for the remaining MCTs. We focus on reflections of 11 students from follow-up semi-structured interviews who experienced both ChatGPT and peer support. Three key themes emerged. First, students observed a Problem Detection Gap: that ChatGPT cannot detect misunderstandings unless explicitly told, unlike peers or teachers. Humans engage in mutual elaboration using talk and their environment  to convey understanding and identify issues. Second was the theme of Premature conclusions as students observed that AI lacks self-doubt capacity, whereas human expressions of uncertainty are crucial for learning and collaborative inquiry. For the third theme, Thinking partner with accountability, relational, and distributed expertise benefits, students highlighted social belonging, accountability, and the benefits of distributed intelligence when working with human peers rather than AI. The findings can inform strategies for deploying LLM-chatbots in education, with hopes of preserving the uniquely human aspects of learning conversations and enlightening how LLM-chatbots could augment and not replace these interactions.
				
				</div>
			</div>
		</div>

		<hr>
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2024_fall/oana_ignat_2024.jpg">
				</div>
				<div>
					<h3> Oana Ignat (Zoom) </h3>
					<p><b> Santa Clara University </b></p>
					<p><b> Time:</b> 10/16/2024, 15:00PM EST - 16:00PM EST</p>
					<p><b> Place:</b> Zoom </p>
					<p><b>Bio</b>: Dr. Oana Ignat is an Assistant Professor in the Computer Science and Engineering (CSE) department at Santa Clara University. She completed her Ph.D. in Computer Science at the University of Michigan.
						Her research interests are at the intersection of Natural Language Processing (NLP) and Computer Vision (CV), where she aims to develop equitable models that work equally well across demographics such as income, race, gender, language, and culture. She is passionate about research applications for social good impact, and the use of AI to attract and retain minorities in CS. Her work resulted in several publications in top conferences such as ACL and EMNLP. She is a co-organizer of the NLP for Positive Impact (NLP4PI) workshop at EMNLP 2024 and the SemEval 2024 Task on Emotion Recognition in Low Resource Languages. Oana is also involved in several outreach programs, including co-organizing the ACL Mentorship global panel sessions and many other events and workshops centered on improving diversity in CS.
					</p>
					<p><b>Title:</b> Towards Inclusive Representations in Language-Vision Models</p>
					<p><b>Abstract:</b>						
						Solving complex real-world problems often requires AI models that can process information from multiple modalities, such as language and vision, which can align with the needs of people from diverse backgrounds. An effective AI model will not only learn how to interact with humans but also do so in a way that reflects the characteristics of those it interacts with, thereby assisting in everyday life activities and significantly improving our quality of life.
						In this talk, I will address the problem of inclusive representations of AI Multimodal Models. I will challenge the common belief that achieving a "general understanding" is possible solely by using English data from Western countries, and I will show how current language-vision models lead to considerable performance gaps across demographics. Finally, I will highlight insights and actionable steps to address these limitations, such as the development of affordable crowdsourced geo-diverse datasets, or flexible labels that consider the data provider.
					</p>
				</div>
			</div>
		</div>

		<hr>
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2024_fall/ben_lipkin_2024.jpg">
				</div>
				<div>
					<h3> Ben Lipkin (Zoom) </h3>
					<p><b> Massachusetts Institute of Technology </b></p>
					<p><b> Time:</b> 10/23/2024, 15:00PM EST - 16:00PM EST</p>
					<p><b> Place:</b> Zoom </p>
					<p><b>Bio:</b> Ben is a 3rd year Ph.D. student in Cognitive Science at MIT, where he is advised by Roger Levy and Ev Fedorenko. His work leverages ideas from modular cognitive architecture and classical NLP/AI to implement robust, reliable, and calibrated natural language systems. He is the recipient of an NSF GRFP fellowship, an Outstanding Paper Award (EMNLP '23), and is the co-organizer of several workshops including Natural Language Reasoning & Structured Explanations (ACL '24). More information can be found on his website: https://benlipkin.github.io/</p>

					<p><b>Title:</b> Symbols and probability in the age of LLMs</p>

					<p><b>Abstract:</b> LLMs have emerged as a dominant paradigm in the design of systems that interact through text. While the capabilities of LLMs in isolation are astounding, some of the most powerful applications have come from their combination with classical symbolic infrastructure, from calculators to game engines, and probabilistic steering, from importance sampling to variational inference. In this talk, I will highlight how leveraging principles from symbolic and probabilistic computation can yield more robust and reliable systems to interact with the world of text. Across a few case studies from my work, I will present approaches that intersect language models with 1) interactive theorem provers to yield provably accurate logical reasoning, 2) probabilistic programming languages to yield uncertainty-aware semantic parsers, and 3) formal grammars to yield sampling algorithms that cannot introduce syntactic (and some semantic) errors, by design. Across these examples, I will show how these augmentations improve core aspects of performance from accuracy to calibration across an array of tasks and will present several early-stage extensions to this research space.</p>
				</div>
			</div>
		</div>

		<hr>
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2024_fall/liyan_tang_2024.png">
				</div>
				<div>
					<h3> Liyan Tang (Zoom) </h3>
					<p><b> The University of Texas at Austin </b></p>
					<p><b> Time:</b> 11/06/2024, 15:00PM EST - 16:00PM EST</p>
					<p><b> Place:</b> Zoom </p>
					<p><b>Bio:</b> Liyan is a fourth-year Ph.D. student in Computer Science from the [TAUR Lab](https://taur.cs.utexas.edu/) (Text Analysis, Understanding, and Reasoning) at UT Austin advised by [Greg Durrett](https://www.cs.utexas.edu/~gdurrett/). He has been fortunate to work with [Ying Ding](https://yingding.ischool.utexas.edu/) from UT iSchool, [Yifan Peng](https://pengyifan.com/) from Weill Cornell Medicine and [Justin F. Rousseau](https://dellmed.utexas.edu/directory/justin-rousseau) from UT Southwestern Medical Center (alphabetical order). His research focuses on text generation and evaluation (especially on hallucination evaluation), as well as their applications in the clinical domain.</p>
				</div>
			</div>
		</div>

		<hr>
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2024_fall/dan_villarreal_2024.jpg">
				</div>
				<div>
					<h3> Dan Villarreal </h3>
					<p><b> University of Pittsburgh </b></p>
					<p><b> Time:</b> 11/20/2024, 15:00PM EST - 16:00PM EST</p>
					<p><b> Place:</b> In-person </p>
					<p><b>Bio:</b></p> Dan Villarreal is a computational sociolinguist, as his scholarly work sits at the nexus of two research traditions: bringing together computational methods and sociolinguistic perspectives. In particular, his research seeks to expand sociolinguists' research toolkits by making computational techniques and sociolinguistic data accessible and usable; explore how speakers and listeners make sense of the tremendous phonetic variability that characterizes everyday speech; and foster a computational sociolinguistics (and a linguistics more broadly) that addresses its research questions faster, better, and more equitably. His recent work has investigated computational methods to automatically code sociophonetic variation (and how to make these methods equitable), gender segregation and speech communities in New Zealand, and whether Open Methods in linguistics contribute to academic colonialism. His research has been published in Language Variation and Change, Laboratory Phonology, and Linguistics Vanguard. Dan pronounces his last name /ˌvɪləɹiˈæl/.
					
					<p><b>Title:</b> Corpus sociolinguistics and methodological trade-offs</p>
					
					<p><b>Abstract:</b>
					The troves of speech data that have driven the increasing orientation toward large-scale methods in sociolinguistics and adjacent subfields have been, for the most part, available only to closed teams of researchers and their collaborators. This trend is beginning to change toward open data resources (e.g., Kendall & Farrington 2023; Stanford 2020). However, these resources tend to be narrowly tailored to specific research questions and/or require substantial additional annotation before researchers can realistically address sociolinguistic research questions.
					In this talk, I introduce a soon-to-be released open data resource—the Archive of Pittsburgh Language and Speech (APLS)—and discuss methodological trade-offs in the process of creating the corpus. As of the time of writing, APLS contains over 32 hours of audio of conversational speech from 34 Pittsburgh English speakers, consisting of over 386,000 word tokens and 900,000 force-aligned segments. (When complete, APLS will include 45 hours of audio from 40 speakers.) Powered by the corpus management software LaBB-CAT (Fromont & Hay 2012), APLS organizes linguistic data into annotation layers that are time-synchronized to speech, from the level of individual speech sounds that may be as short as 30 milliseconds, all the way to the level of entire hourlong interviews. I discuss how the methods for creating the corpus have evolved over the course of the APLS project, such as the introduction of ASR tools for transcription.</p>

				</div>
			</div>
		</div>
			
		<div class="row">
		<div class="col-md-12 text-left">
			<div class="speaker-pic" style="display: inline-block">
				<img src="img/speakers/2024_winter/Zhijing_Talk.jpg">
			</div>
			<div>
			<h3> Zhijing Jin (Remote) [<a href="https://drive.google.com/file/d/1Q8SJDgan4mGQJzPoDS3Zu2JbgTyAJ6gz/view?usp=sharing">Recording</a>]</h3>
			<p><b> Max Planck Institute & ETH </b></p>
			<p><b> Time:</b> 2/15/2024, 16:00am EST - 17:00am EST </p>
			<!-- <p><b> Place:</b><a href="https://pitt.zoom.us/j/99491956988"> Zoom</a> and 538-539 in 130 N Bellefield Ave</p> -->
			<p><b> Topic:</b> Causal Inference in NLP </p>
			<p><b> Title:</b> Causal Inference for Robust, Reliable, and Responsible NLP </p>
			<p><b>Abstract:</b> Despite the remarkable progress in large language models (LLMs), it is well-known that natural language processing (NLP) models tend to fit for spurious correlations, which can lead to unstable behavior under domain shifts or adversarial attacks. In my research, I develop a causal framework for robust and fair NLP, which investigates the alignment of the causality of human decision-making and model decision-making mechanisms. Under this framework, I develop a suite of stress tests for NLP models across various tasks, such as text classification, natural language inference, and math reasoning; and I propose to enhance robustness by aligning model learning direction with the underlying data generating direction. Using this causal inference framework, I also test the validity of causal and logical reasoning in models, with implications for fighting misinformation, and also extend the impact of NLP by applying it to analyze the causality behind social phenomena important for our society, such as causal analysis of policies, and measuring gender bias in our society. Together, I develop a roadmap towards socially responsible NLP by ensuring the reliability of models, and broadcasting its impact to various social applications. </p>
			<p><b>Bio:</b> Zhijing Jin (she/her) is a Ph.D. candidate at Max Planck Institute & ETH. Her research focuses on socially responsible NLP by causal inference. Specifically, she works on expanding the impact of NLP by promoting NLP for social good, and developing CausalNLP to improve robustness, fairness, and interpretability of NLP models, as well as analyze the causes of social problems. She has published at many NLP and AI venues (e.g., ACL, EMNLP, NAACL, NeurIPS, AAAI, AISTATS). Her work has been featured in MIT News, ACM TechNews, and Synced. She is actively involved in AI for social good, as the co-organizer of three NLP for Positive Impact Workshops (at ACL 2021, EMNLP 2022, and EMNLP 2024), Moral AI Workshop at NeurIPS 2023, and RobustML Workshop at ICLR 2021. To support the NLP research community, she organizes the ACL Year-Round Mentorship Program. To foster the causality research community, she organized the Tutorial on CausalNLP at EMNLP 2022, and served as the Publications Chair for the 1st conference on Causal Learning and Reasoning (CLeaR). More information can be found on her personal website: zhijing-jin.com</p>
		</div>

		<hr>
		<div class="row">
		<div class="col-md-12 text-left">
			<div class="speaker-pic" style="display: inline-block">
				<img src="img/speakers/2024_winter/AARON_MUELLER.jpeg">
			</div>
			<div>
			<h3> Aaron Mueller (Remote)</h3>
			<p><b> Khoury College of Computer Sciences, Northeastern University </b></p>
			<p><b> Time:</b> 2/29/2024, 16:00am EST - 17:00am EST</p>
			<p><b> Place:</b><a href="https://pitt.zoom.us/j/99491956988"> Zoom</a> and 5th floor in 130 N Bellefield Ave</p>
			<p>
				<b>Title</b>: Evaluating and Surgically Improving Generalization in Language Models
			</p>
			<p>
				<b>Abstract</b>: As language models (LMs) are deployed in wider applications, understanding and controlling how they generalize becomes increasingly important. However, it is difficult to directly evaluate how models are accomplishing the tasks we give them—and when needed, it is not obvious how to improve generalization on a task without destroying general capabilities. In this talk, I will present two projects that tackle these challenges. I will first present an evaluation of how models process language structure: we evaluate out-of-domain generalization in in-context learning settings, finding that pre-training on code may result in more robust generalization. We also find that chain-of-thought (CoT) results can be misleading: CoT often only improves in-distribution performance without improving out-of-distribution performance. Then, I will present an ongoing mechanistic interpretability effort to isolate and control the algorithms LMs implement via feature circuits. By learning sparse human-interpretable encodings of models’ hidden states (features) and discovering circuits on them, we observe how LMs perform subject-verb agreement: by composing representations of grammatical number in the MLPs and residuals, while detecting and learning to ignore distractor clauses in the attention heads. I will conclude by showing an application of feature circuits—ablating spurious features to improve the generalization of a classifier.
			</p>
			<p>
				<b>Bio</b>: Aaron Mueller is a Zuckerman postdoctoral fellow working with David Bau (Northeastern U.) and Yonatan Belinkov (Technion). He obtained his PhD from Johns Hopkins University supervised by Tal Linzen. His work spans topics in the intersection of natural language processing and psycholinguistics, including causal interpretability, NLP evaluations inspired by linguistic principles, and efficient language acquisition. He was an NSF Graduate Fellow, and has received an Outstanding Paper Award from ACL (2023), a Featured Paper recognition from TMLR (2023), and coverage in the New York Times as an organizer of the BabyLM Challenge.
			</p>

		</div>
		<hr>
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2024_winter/muhammad_khalifa_2024.png">
				</div>
				<div>
					<h3> Muhammad Khalifa (Remote)</h3>
					<p><b> University of Michigan in Ann Arbor</b></p>
					<p><b> Time:</b> 3/21/2024, 16:00am EST - 17:00am EST</p>
					<p><b> Place:</b><a href="https://pitt.zoom.us/j/99491956988"> Zoom</a> and 5th floor in 130 N Bellefield Ave</p>
					<p><b>Bio:</b> Muhammad Khalifa is a third-year PhD candidate at the University of Michigan in Ann Arbor and an intern at Ai2. He is advised by Lu Wang and Honglak Lee. His main research interests are Large Language Models, Reasoning, and Controlled Generation. He spent 10 months at Amazon AI working with Miguel Ballesteros and Kathy Mckeown on multiple projects including Dialogue Summarization and Semi-structured documents understanding. Prior to that, hw was an intern at Naver Labs Europe where he worked on Controllable Text Generation and Energy-based models with Hady Elsahar and Marc Dymetman.</p>
				</div>
			</div>
		</div>
		
		
		<hr>
		<div class="row">
		<div class="col-md-12 text-left">
			<div class="speaker-pic" style="display: inline-block">
				<img src="img/speakers/2024_winter/Ana_Marasovic.jpg">
			</div>
			<div>
			<h3> Ana Marasović (Remote) </h3>
			<p><b><a href="https://docs.google.com/presentation/d/1ZMEiAsBkSG8VqNe34VKvuOJZ4UtGIUd5dw67A92Xhpk/edit#slide=id.p">Slide</a></b></p>
			<p><b> the Kahlert School of Computing at the University of Utah </b></p>
			<p><b> Time:</b> 4/4/2024, 16:00am EST - 17:00am EST</p>
			<p><b> Place:</b><a href="https://pitt.zoom.us/j/99491956988"> Zoom</a> and 5th floor in 130 N Bellefield Ave</p>
			<p><b>Bio:</b> Ana Marasović is an Assistant Professor in the Kahlert School of Computing at the University of Utah. Her primary research interests are at the confluence of natural language processing (NLP), explainable artificial intelligence (XAI), and multimodality. She is interested in projects that (1) rigorously validate AI technologies, and (2) make human interaction with AI more intuitive.
						For an example of robust validation check out her work on carefully designing benchmarks to validate the robustness of QA models in the presence of common linguistic phenomena such as negation or coreference. On the other hand, to help people create a mental model about how to interact with AI, she has contributed to building models that self-explain their predictions in a way that is easily understandable to people, for example by saying why did the model give this answer instead of another one (contrastive explanations) or by telling in plain language the gist of its reasoning (free-text explanations). Moving forward, she is excited to evaluate and improve such models with application-grounded, human-subject evaluations.
						Previously, Ana Marasović was a Young Investigator at the Allen Institute for AI from 2019–2022 where she worked with Noah A. Smith and Yejin Choi. During that time she also had a courtesy appointment in the Paul G. Allen School of Computer Science & Engineering at the University of Washington. Ana Marasović received her Ph.D. degree at Heidelberg University, where she was advised by Anette Frank. Before receiving her PhD in 2019, she completed her B.Sc. (2013) and M.Sc. (2015) in Mathematics at the University of Zagreb.
						</p>
		</div>
		<hr>

		<div class="row">
			<div class="col-md-12 text-left">
				<!-- <div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/lizhang.jpeg">
				</div> -->
				<div>
					<h3> Joel Tetreault (Remote) </h3>
					<p><b> Dataminr </b></p>
					<p>
						<b> Time:</b> 4/11/2024, 16:00am EST - 17:00am EST
					</p>
					<p>
						</b><a href="https://pitt.zoom.us/j/99491956988"> Zoom</a> and 5th floor in 130 N Bellefield Ave</p>
					</p>
					<!-- <p><b> Topic:</b> Causal Inference in NLP </p> -->
					<p><b> Title:</b> A Brief History of Natural Language Processing </p>
					<p>
						<b>Abstract: </b>  The title says it all!  As the field of Natural Language Processing (NLP) continues to make incredible strides and advancements, it's important to take a step back and use the past to understand the current transformations.  Drawing from literature and interviews, we'll dive into the early years of NLP and explore some of the major themes, trends, and personalities that paved the way for the cutting-edge technology we have today.
					
					</p>
					<p>
						<b>Bio:</b> Joel Tetreault is VP of Research at Dataminr, a company that provides updates on breaking events across the world in real-time. His background is in AI, specifically Natural Language Processing and Machine Learning, and using techniques from those fields to solve real-world problems such as automatic essay scoring, grammatical error correction, hate speech detection, real-time event detection, and dialogue systems, AI for Good, among others.  Prior to joining Dataminr, he led research groups at Grammarly, Nuance, and Educational Testing Service, and was a Senior Research Scientist at Yahoo Labs.   Joel was one of the program chairs of ACL 2020 and also one of the longest-serving members of the NAACL Board where he was Treasurer for six years.  Additionally, he was a long-time organizer of the Building Educational Application workshop series (10+ years) and organized workshops on Generation, AI for Social Good, Abusive Language, Metaphor and Event Detection.
					</p>
				</div>
			</div>
		</div>

		<hr>
		
		<div class="row">
			<div class="col-md-12 text-left">
				<div class="speaker-pic" style="display: inline-block">
					<img src="img/speakers/2024_winter/lizhang.jpeg">
				</div>
				<div>
					<h3> Zhang "Harry" Li (Remote)</h3>
					<p><b> University of Pennsylvania</b></p>
					<p>
						<b> Time:</b> 4/18/2024, 16:00am EST - 17:00am EST
					</p>
					<p>
						</b><a href="https://pitt.zoom.us/j/99491956988"> Zoom</a> and 5th floor in 130 N Bellefield Ave</p>
					</p>
					<!-- <p><b> Topic:</b> Causal Inference in NLP </p> -->
					<p><b> Title:</b> Structured Event Reasoning with Large Language Models </p>
					<p>
						<b>Abstract: </b>  Reasoning about real-life events is a unifying challenge in AI and NLP that has profound utility in a variety of domains, while any fallacy in high-stake applications like law, medicine, and science could be catastrophic. Able to work with diverse text in these domains, large language models (LLMs) have proven capable of answering questions and solving problems. In this talk, I demonstrate that end-to-end LLMs still systematically fail on reasoning tasks of complex events. Moreover, their black-box nature gives rise to little interpretability and user control. To address these issues, I propose two general approaches to use LLMs in conjunction with a structured representation of events. The first is a language-based representation involving relations of sub-events that can be learned by LLMs via fine-tuning. The second is a symbolic representation involving states of entities that can be leveraged by either LLMs or deterministic solvers. On a suite of event reasoning tasks, I show that both approaches outperform end-to-end LLMs in terms of performance and trustworthiness. 
					</p>
					<p>
						<b>Bio:</b> Li "Harry" Zhang is a 5th-year PhD student working on Natural Language Processing (NLP) and artificial intelligence at the University of Pennsylvania advised by Prof. Chris Callison-Burch. He earned his Bachelor's degree at the University of Michigan mentored by Prof. Rada Mihalcea and Prof. Dragomir Radev. He has published more than 20 papers in NLP conferences that have been cited more than 1,000 times. He has reviewed more than 50 papers in those venues and has served as Session Chair and Program Chair in many conferences and workshops. Being also a musician, producer, content creator of over 50,000 subscribers, he is passionate in the research of AI music. 
					</p>
				</div>
			</div>
		</div>
			


	</div>
	</section>

<div class="container2">

<form action="mailto:pittnlpseminar@list.pitt.edu" enctype="text/plain" method="POST">

  <div class ="email-box">
		<i class="fa fa-envelope" > </i>
		<input class="tbox" type="email" name="email" value="" placeholder="Enter your e-mail" required>
		<button class="btn" type="submit"  name="button"> Subscribe </button>
  </div>
 </form >
 </div>

	<div id="sub-footer">
		<div class="container">
			<div class="row">
				<div class="col-lg-6">
					<div class="copyright">
						<p>
							<span>&copy; All rights reserved </span>
						</p>
					</div>
				</div>
				<div class="col-lg-6">
					<ul class="social-network">
						<!--
						<li><a href="#" data-placement="top" title="Facebook"><i class="fa fa-facebook"></i></a></li>
						<li><a href="#" data-placement="top" title="Twitter"><i class="fa fa-twitter"></i></a></li>
						<li><a href="#" data-placement="top" title="Linkedin"><i class="fa fa-linkedin"></i></a></li>
						<li><a href="#" data-placement="top" title="Pinterest"><i class="fa fa-pinterest"></i></a></li>
						<li><a href="#" data-placement="top" title="Google plus"><i class="fa fa-google-plus"></i></a></li>
						-->
					</ul>
				</div>
			</div>
		</div>
	</div>
	</footer>
</div>

<!-- Something else -->
<a href="#" class="scrollup"><i class="fa fa-angle-up active"></i></a>
<!-- javascript
    ================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="js/jquery.js"></script>
<script src="js/jquery.easing.1.3.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/jquery.fancybox.pack.js"></script>
<script src="js/jquery.fancybox-media.js"></script>
<script src="js/portfolio/jquery.quicksand.js"></script>
<script src="js/portfolio/setting.js"></script>
<script src="js/jquery.flexslider.js"></script>
<script src="js/animate.js"></script>
<script src="js/custom.js"></script>
<script src="js/owl-carousel/owl.carousel.js"></script>
</body>
</html>
